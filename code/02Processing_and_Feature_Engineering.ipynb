{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de3bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab78915",
   "metadata": {},
   "source": [
    "#### In this notebook, I will create several new columns in our data representing post characteristics, as well as tokenize and lemmatize the text. The dataframes will be combined into one and shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ba629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44291269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "sa_data = pd.read_csv('../data/SA_data.csv')\n",
    "gt_data = pd.read_csv('../data/GT_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81f01cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnamed columns\n",
    "sa_data.drop(columns='Unnamed: 0', inplace=True)\n",
    "gt_data.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f81da2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make post text lower case\n",
    "sa_data['selftext'] = sa_data['selftext'].str.lower()\n",
    "gt_data['selftext'] = gt_data['selftext'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "144fcc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selftext                                 a\n",
       "subreddit               Stormlight_Archive\n",
       "title                          Test2 (row)\n",
       "tokenized_text                           a\n",
       "tokenized_text_clean                      \n",
       "char_count                               1\n",
       "word_count                               1\n",
       "tagged_tokens                           []\n",
       "lemmatized_words                          \n",
       "Name: 116, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_data.loc[116,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f49953",
   "metadata": {},
   "source": [
    "### Add Tokenized Text, Word Count and Character Count Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aac14bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f68479f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize sa selftext into new columns\n",
    "for i in range(len(sa_data)):\n",
    "    sa_data.loc[i,'tokenized_text'] = ' '.join(tokenizer.tokenize(sa_data.loc[i,'selftext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a311b934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize gt selftext into new columns\n",
    "for i in range(len(gt_data)):\n",
    "    gt_data.loc[i,'tokenized_text'] = ' '.join(tokenizer.tokenize(gt_data.loc[i,'selftext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0359bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove english stop words from tokenized text\n",
    "stop_words = stopwords.words('english')\n",
    "gt_data['tokenized_text_clean'] = gt_data['tokenized_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "sa_data['tokenized_text_clean'] = sa_data['tokenized_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ea0e89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add post character count column\n",
    "gt_data['char_count'] = gt_data['selftext'].apply(lambda x: len(x))\n",
    "sa_data['char_count'] = sa_data['selftext'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b3f8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add post word count column\n",
    "gt_data['word_count'] = gt_data['selftext'].apply(lambda x: len(x.split()))\n",
    "sa_data['word_count'] = sa_data['selftext'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8428ad9c",
   "metadata": {},
   "source": [
    "### Combine Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4a849627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Stormlight and ASOIAF data into book data\n",
    "book_data = pd.concat([gt_data, sa_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "07448064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shuffle book data so that classes are not stratified\n",
    "book_data = book_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd0b69",
   "metadata": {},
   "source": [
    "### Lemmatize Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5ffa4f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Lemmatizer\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5785f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag tokens with part of speach symbol\n",
    "book_data['tagged_tokens'] = book_data['tokenized_text_clean'].apply(lambda x: nltk.pos_tag(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "04d76ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing function written collaboratively with DSI-919 cohort\n",
    "def custom_lemmatize(word, tag):\n",
    "    mapper = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    pos = mapper.get(tag[0])\n",
    "    \n",
    "    return wn.lemmatize(word, pos) if pos else word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1ee2a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make column of lemmatized words\n",
    "book_data['lemmatized_words'] = book_data['tagged_tokens'].apply(lambda x: ' '.join([custom_lemmatize(word, tag) for word, tag in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c7c992f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write new dataframe to csv\n",
    "book_data.to_csv('../data/book_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
